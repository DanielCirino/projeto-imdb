{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtFHRxcH3nTR"
   },
   "source": [
    "![header-notebooks.png](data:image/png;base64,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)\n",
    "\n",
    "Pr√°tica da **Aula 8: Spark e Fontes de Dados**, do curso de **Engenharia de Dados** da **[Awari](https://awari.com.br/)**. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import date, datetime\n",
    "import pyspark\n",
    "import boto3\n",
    "\n",
    "from io import StringIO \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
    "from pyspark.sql.functions import col,array_contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "m2RXTtet4lm7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://40f0d7cc57a8:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>AwariAula08</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x250c62b9550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Criando uma sess√£o com o Spark que existe localmente(atualmente configurado junto com o JupyterLab)\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[*]\") \\\n",
    "      .appName(\"AwariAula08\") \\\n",
    "      .config(\"spark.memory.offHeap.enabled\",\"true\") \\\n",
    "      .config(\"spark.memory.offHeap.size\",\"10g\") \\\n",
    "      .getOrCreate() \n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um RDD\n",
    "- Resilient Distributed Datasets, abstraem um conjunto de objetos distribu√≠dos no cluster, geralmente executados na mem√≥ria principal e podem depois serem salvos em um Haddop, Banco de Datos, arquivo, etc, quando voc√™ carrega \n",
    "- Um RDD ele √© um dado IMUTAVEL e e semelhante a uma lista em Python, com a diferen√ßa de que o RDD √© calculado em v√°rios processos espalhados por v√°rios servidores f√≠sicos, tamb√©m chamados de n√≥s em um cluster, enquanto uma cole√ß√£o Python vive e processa em apenas um processo.\n",
    "- RDD √© mais utilizado quando precisamos trabalhar mais baixo nivel manipulando diretamente o DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o39.defaultParallelism.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:484)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1623)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)\r\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2559)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Partir da memoria\u001b[39;00m\n\u001b[0;32m      2\u001b[0m dataList \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m20000\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m100000\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScala\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m3000\u001b[39m)]\n\u001b[1;32m----> 3\u001b[0m rdd\u001b[38;5;241m=\u001b[39m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataList\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Awari\\EngenhariaDados\\projeto-imdb\\venv\\Lib\\site-packages\\pyspark\\context.py:780\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[1;34m(self, c, numSlices)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallelize\u001b[39m(\u001b[38;5;28mself\u001b[39m, c: Iterable[T], numSlices: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RDD[T]:\n\u001b[0;32m    749\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;124;03m    Distribute a local Python collection to form an RDD. Using range\u001b[39;00m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;124;03m    is recommended if the input represents a range for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;124;03m    [['a'], ['b', 'c']]\u001b[39;00m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 780\u001b[0m     numSlices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(numSlices) \u001b[38;5;28;01mif\u001b[39;00m numSlices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefaultParallelism\u001b[49m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, \u001b[38;5;28mrange\u001b[39m):\n\u001b[0;32m    782\u001b[0m         size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(c)\n",
      "File \u001b[1;32m~\\Documents\\Awari\\EngenhariaDados\\projeto-imdb\\venv\\Lib\\site-packages\\pyspark\\context.py:627\u001b[0m, in \u001b[0;36mSparkContext.defaultParallelism\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefaultParallelism\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    617\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;124;03m    Default level of parallelism to use when not given by user (e.g. for reduce tasks)\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefaultParallelism\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Awari\\EngenhariaDados\\projeto-imdb\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\Documents\\Awari\\EngenhariaDados\\projeto-imdb\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\Documents\\Awari\\EngenhariaDados\\projeto-imdb\\venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o39.defaultParallelism.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:484)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1623)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)\r\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2559)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n"
     ]
    }
   ],
   "source": [
    "# Partir da memoria\n",
    "dataList = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "rdd=spark.sparkContext.parallelize(dataList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A Partir de um arquivo\n",
    "rdd2 = spark.sparkContext.textFile(\"./arquivos/customers.csv\")\n",
    "header = rdd2.first()\n",
    "rdd2 = rdd2.filter(lambda row: row != header)\n",
    "rdd2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Opera√ß√µes\n",
    "Transforma√ß√µes no Spark etorna outro RDD e essas transforma√ß√µes s√£o lentas, o que significa que elas n√£o s√£o executadas at√© que voc√™ chame uma a√ß√£o no RDD. Algumas transforma√ß√µes em RDD's s√£o flatMap(), map(), reduceByKey(), filter(), sortByKey() e retornam um novo RDD ao inv√©s de atualizar o atual.\n",
    "\n",
    "# RDD A√ß√µes\n",
    "Retorna os valores de um RDD para um n√≥ de driver. Em outras palavras, qualquer fun√ß√£o RDD que retorna um n√£o RDD √© considerada uma a√ß√£o. Algumas a√ß√µes no RDD s√£o count(), collect(), first(), max(), reduce() and more.\n",
    "\n",
    "# PySpark DataFrame\n",
    "DataFrame √© uma cole√ß√£o distribu√≠da de dados organizados em colunas nomeadas. √â conceitualmente equivalente a uma tabela em um banco de dados relacional ou um quadro de dados em R/Python, mas com otimiza√ß√µes mais ricas sob o cap√¥. Os DataFrames podem ser constru√≠dos a partir de uma ampla variedade de fontes, como arquivos de dados estruturados, tabelas no Hive, bancos de dados externos ou RDDs existentes. Diferente do RDD √© mais utilizado para analise de dados, debugs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Criando um DataFrame\n",
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "print(df.show(), df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lendo um arquivo\n",
    "df = spark.read.option(\"header\", True).csv(\"./arquivos/customers.csv\")\n",
    "print(df.show(), df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquivos suportados por um PySpark DataFrame\n",
    "\n",
    "- csv\n",
    "- text\n",
    "- Avro\n",
    "- Parquet\n",
    "- tsv\n",
    "- xml e muito mais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"CUSTOMERS\")\n",
    "df2 = spark.sql(\"SELECT * from CUSTOMERS LIMIT 5\")\n",
    "print(df2.show(), df2.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Usando GROUPBY\n",
    "groupDF = spark.sql(\"SELECT Profession, count(*) as total from CUSTOMERS group by Profession\")\n",
    "groupDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "\n",
    "# Criando uma sess√£o com o Spark que existe localmente(atualmente configurado junto com o JupyterLab)\n",
    "conf.setMaster(\"local[1]\") \n",
    "conf.set(\"spark.driver.host\", \"awari-jupyterlab\") \\\n",
    "    .set(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\") \\\n",
    "    .set(\"parquet.enable.summary-metadata\", \"false\") \\\n",
    "    .set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\") \\\n",
    "    .set(\"spark.driver.port\", \"20020\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", 'awari-nginx:9000') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint.region\", 'sa-east-1') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", 'mnYOiUf07UBjjJwf') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", '1Qu7X3EmbIYDNXUiuvFSDUJwJ4fWdyT5') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .set(\"spark.hadoop.com.amazonaws.services.s3.enableV2\", \"true\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.committer.staging.conflict-mode\", \"replace\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.committer.name\", \"directory\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.committer.staging.tmp.path\", \"/tmp/staging\")\n",
    "\n",
    "conf.setAppName('AwariAula08-S33')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('s3', \n",
    "    endpoint_url='http://awari-minio-nginx:9000',\n",
    "    aws_access_key_id='mnYOiUf07UBjjJwf',\n",
    "    aws_secret_access_key='1Qu7X3EmbIYDNXUiuvFSDUJwJ4fWdyT5',\n",
    "    aws_session_token=None,\n",
    "    config=boto3.session.Config(signature_version='s3v4'),\n",
    "    verify=False,\n",
    "    region_name='sa-east-1'\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", True).csv(\"s3a://aula-08/resources/usuarios.csv\")\n",
    "print(df.show(), df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Usando RDD\n",
    "# A Partir de um arquivo\n",
    "rdd3 = spark.sparkContext.textFile(\"s3a://aula-08/resources/usuarios.csv\")\n",
    "rdd3 = rdd3.map( lambda f: f.split(\",\"))\n",
    "header = rdd3.first()\n",
    "\n",
    "rdd3 = rdd3.filter(lambda row: row != header)\n",
    "rdd3 = sc.parallelize(rdd3.collect())\n",
    "df = rdd3.toDF(header)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convertendo CSV para PARQUET e salvando no storage\n",
    "df.write.save(\"s3a://aula-08/meu-parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lendo um parquet file\n",
    "parDF1=spark.read.parquet(\"s3a://aula-08/meu-parquet\")\n",
    "parDF1.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame\n",
    "Diferente de Series, **DataFrame** possui uma estrutura bidemensial, como uma planilha de Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'macas': [3, 2, 0, 1], \n",
    "    'laranjas': [0, 3, 7, 2]\n",
    "}\n",
    "\n",
    "compras = pd.DataFrame(data)\n",
    "\n",
    "compras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compras = pd.DataFrame(data, index=['Alex', 'Roberto', 'Bernardo', 'Paulo'])\n",
    "\n",
    "compras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para extrair apenas uma Serie\n",
    "compras.loc['Alex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lendo informa√ß√µes de um CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('arquivos/customers.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apenas os 5 primeiros registros\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quando carregamos um CSV os indices s√£o gerados aleatoriamente por ordem da linha \n",
    "# caso queiramos especificar um indice, precisaremos informar qual a coluna √© o indice\n",
    "\n",
    "df = pd.read_csv('arquivos/customers.csv', index_col=0)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lendo arquivos em JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('./arquivos/estados.json', orient='records')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo dados de uma URL\n",
    "url = \"https://api.exchangerate-api.com/v4/latest/USD\"\n",
    "df = pd.read_json(url)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvando DataFrame em JSON ou CSV\n",
    "\n",
    "Podemos, usando a mesma l√≥gica, carregar um arquivo JSON salvar em CSV e vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simples formato\n",
    "df = pd.DataFrame([1,2,3])\n",
    "print(df)\n",
    "df.to_json(\"./arquivos/numeros_json.json\")\n",
    "df.to_csv(\"./arquivos/numeros_csv.json\")\n",
    "!ls ./arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([1,2,3], columns=['Numeros'])\n",
    "df.to_json(\"./arquivos/numeros_json2.json\")\n",
    "df.to_csv(\"./arquivos/numeros_csv2.csv\")\n",
    "!ls ./arquivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratando um arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./arquivos/4300Answers.csv')\n",
    "df.head(2) # Checar colunas com NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo colunas NaN para \"-\"\n",
    "df = df.fillna(\"-\") # N√≥s precisamos atrelar o resultado a variavel principal\n",
    "df.head(2) # Checar colunas novamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retornando apenas o sexo Feminino\n",
    "resultado_query=df.query(\"`Qual seu sexo?` == 'Feminino'\")\n",
    "resultado_query.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando novo Json apenas com os resultados femininos\n",
    "resultado_query.to_csv(\"./arquivos/4300Answers_femininos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo arquivo gerado\n",
    "df = pd.read_csv(\"./arquivos/4300Answers_femininos.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAH7iNcj4PEE"
   },
   "source": [
    "---\n",
    "\n",
    "Notebook utilizado para fins educacionais da **Awari**.\n",
    "\n",
    "**¬© AWARI. Todos os direitos reservados.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMpmB1/ZiTM76Kc2DEBlVuc",
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1WPJzzm-0hWc6Q5JetcdpWbJrsPesiWLs",
     "timestamp": 1666129304753
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
