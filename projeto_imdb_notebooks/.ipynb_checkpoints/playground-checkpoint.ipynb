{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a4f047a-5cde-4535-b711-499ab3158b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import boto3\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08640c07-2eb0-4a7b-87b4-6287a3905307",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format=\"[%(levelname)s] [%(asctime)s] %(message)s\",\n",
    "                    level=logging.WARN,\n",
    "                    datefmt=\"%d/%m/%y %H:%M:%S\",\n",
    "                    encoding=\"utf-8\")\n",
    "\n",
    "AWS_ENDPOINT = \"http://minio-webserver:9000\"\n",
    "AWS_ACCESS_KEY_ID = \"U6dHDkvTv3CdrviA\"\n",
    "AWS_SECRET_ACCESS_KEY = \"o8LTVjrVtNuzOv7DjGOxhN0HQDeksEej\"\n",
    "AWS_REGION =\"us-east-1\"\n",
    "\n",
    "SPARK_APP_NAME= \"projeto-imdb-notebook\"\n",
    "SPARK_MASTER_URL=\"spark://spark-master:7077\"\n",
    "\n",
    "BUCKET_ANALYTICS=\"projeto-imdb-analytics\"\n",
    "BUCKET_STAGE=\"projeto-imdb-stage\"\n",
    "\n",
    "\n",
    "def obterJars():\n",
    "    dir = \"/notebooks/spark-jars\"\n",
    "    jars = os.listdir(dir)\n",
    "    stringJars = \"\"\n",
    "\n",
    "    for jar in jars:\n",
    "        logging.info(f\"{dir}/{jar}\")\n",
    "        stringJars += f\"{dir}/{jar},\"\n",
    "\n",
    "    return stringJars[:-1]\n",
    "\n",
    "def salvarDataFrameBancoDados(df,tabela):\n",
    "    df.write.format(\"jdbc\")\\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres-server:5432/projeto_imdb\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", tabela) \\\n",
    "    .option(\"user\", \"postgres\").option(\"password\", \"postgres\").save(mode=\"overwrite\")\n",
    "\n",
    "    print(f\"{df.count()} registros salvos na tabela {tabela}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b169ddc2-8238-4201-b3f4-88fa82296918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://9c94b2394cd2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>projeto-imdb-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc01cdb0750>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_client.stop()\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setAppName(SPARK_APP_NAME)\n",
    "conf.setMaster(SPARK_MASTER_URL)\n",
    "\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", AWS_ENDPOINT) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint.region\", AWS_REGION) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.access.key\",AWS_ACCESS_KEY_ID) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", False) \\\n",
    "        .set(\"spark.hadoop.com.amazonaws.services.s3.enableV2\", True) \\\n",
    "        .set(\"spark.jars\", obterJars()) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.committer.staging.conflict-mode\", \"replace\") \\\n",
    "        .set(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .set(\"spark.sql.sources.commitProtocolClass\",\n",
    "             \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\") \\\n",
    "        .set(\"spark.executor.memory\", \"2g\") \\\n",
    "        .set(\"spark.driver.memory\", \"2g\") \\\n",
    "        .set(\"spark.cores.max\", \"4\") \n",
    "        # .set(\"spark.hadoop.fs.s3a.committer.name\", \"magic\") \\\n",
    "        # .set(\"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\", \"true\")\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf).getOrCreate()\n",
    "sc.setLogLevel(\"warn\")\n",
    "spark_client = SparkSession(sc)\n",
    "spark_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c288c4-2d0a-4465-9fba-5a4ccc0a0e6b",
   "metadata": {},
   "source": [
    "# Criar as tabelas temporÃ¡rias a partir dos arquivos *.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f9983de-435b-4128-b5ee-23356ea84486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/21 00:28:35 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nconst: string (nullable = true)\n",
      " |-- primaryName: string (nullable = true)\n",
      " |-- birthYear: string (nullable = true)\n",
      " |-- deathYear: string (nullable = true)\n",
      " |-- primaryProfession: string (nullable = true)\n",
      " |-- knownForTitles: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNameBasics = spark_client \\\n",
    "        .read \\\n",
    "        .parquet(f\"s3a://{BUCKET_ANALYTICS}/name_basics\", header=True)\n",
    "\n",
    "dfNameBasics.createOrReplaceTempView(\"name_basics\")\n",
    "dfNameBasics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fad49ccb-4fe1-452c-bfdd-4efd9e3c81fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- titleId: string (nullable = true)\n",
      " |-- ordering: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- types: string (nullable = true)\n",
      " |-- attributes: string (nullable = true)\n",
      " |-- isOriginalTitle: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfAkas = spark_client \\\n",
    "        .read \\\n",
    "        .parquet(f\"s3a://{BUCKET_ANALYTICS}/title_akas\", header=True)\n",
    "\n",
    "dfAkas.createOrReplaceTempView(\"title_akas\")\n",
    "dfAkas.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0f14216-8b7d-4061-990f-5a353bdda96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- titleType: string (nullable = true)\n",
      " |-- primaryTitle: string (nullable = true)\n",
      " |-- originalTitle: string (nullable = true)\n",
      " |-- isAdult: string (nullable = true)\n",
      " |-- startYear: string (nullable = true)\n",
      " |-- endYear: string (nullable = true)\n",
      " |-- runtimeMinutes: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTitleBasics = spark_client \\\n",
    "        .read \\\n",
    "        .parquet(f\"s3a://{BUCKET_ANALYTICS}/title_basics\", header=True)\n",
    "\n",
    "dfTitleBasics.createOrReplaceTempView(\"title_basics\")\n",
    "dfTitleBasics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0b56269-f55c-4b6e-8b8f-5bc6320c59e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- directors: string (nullable = true)\n",
      " |-- writers: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfTitleCrew = spark_client \\\n",
    "        .read \\\n",
    "        .parquet(f\"s3a://{BUCKET_ANALYTICS}/title_crew\", header=True)\n",
    "\n",
    "dfTitleCrew.createOrReplaceTempView(\"title_crew\")\n",
    "dfTitleCrew.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "696ab5c7-51fc-4c2e-a91b-97095586bf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- parentTconst: string (nullable = true)\n",
      " |-- seasonNumber: string (nullable = true)\n",
      " |-- episodeNumber: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTitleEpisode = spark_client \\\n",
    "        .read \\\n",
    "        .parquet(f\"s3a://{BUCKET_ANALYTICS}/title_episode\", header=True)\n",
    "\n",
    "dfTitleEpisode.createOrReplaceTempView(\"title_episode\")\n",
    "dfTitleEpisode.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "109a5191-cb36-4d93-9a18-e624a773c9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- ordering: string (nullable = true)\n",
      " |-- nconst: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- characters: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTitlePrincipals = spark_client \\\n",
    "        .read \\\n",
    "        .parquet(f\"s3a://{BUCKET_ANALYTICS}/title_principals\", header=True)\n",
    "\n",
    "dfTitlePrincipals.createOrReplaceTempView(\"title_principals\")\n",
    "dfTitlePrincipals.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "198a1f90-0dfb-4a75-94c4-caff96d705bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- averageRating: string (nullable = true)\n",
      " |-- numVotes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfRatings = spark_client \\\n",
    "        .read \\\n",
    "        .parquet(f\"s3a://{BUCKET_ANALYTICS}/title_ratings\", header=True)\n",
    "\n",
    "dfRatings.createOrReplaceTempView(\"title_ratings\")\n",
    "dfRatings.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efdd61c-b98d-4ba6-96c2-2d8a32e66ce3",
   "metadata": {},
   "source": [
    "# Extrair dados da regiÃ£o do Brasil para anÃ¡lise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc9d9cc6-7ef7-4344-9cab-a579e3546e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existem 111049 titulos que foram lanÃ§ados no Brasil\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#TÃ­tulos que possuem lanÃ§amentos no Brasil\n",
    "sqlTitulosBR = \"\"\"\n",
    "    select distinct titleId\n",
    "    from title_akas \n",
    "    where 1=1\n",
    "    and region='BR' \n",
    "    --group by titleId having count(*)=1\n",
    "    order by titleId\n",
    "\n",
    "\"\"\"\n",
    "dfTitulosBR = spark_client.sql(sqlTitulosBR)\n",
    "dfTitulosBR.createOrReplaceTempView(\"title_br\")\n",
    "print(f\"Existem {dfTitulosBR.count()} titulos que foram lanÃ§ados no Brasil\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc56a809-8013-4199-b382-9e9e63d95f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'diretorioArquivo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124m    select ta.* \u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    from title_akas ta \u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m        inner join title_br tbr \u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m        on ta.titleId=tbr.titleId\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      7\u001b[0m dfAkasBr \u001b[38;5;241m=\u001b[39m spark_client\u001b[38;5;241m.\u001b[39msql(sql)\n\u001b[0;32m----> 8\u001b[0m \u001b[43msalvarDataFrameBancoDados\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfAkasBr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitle_akas_br\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 35\u001b[0m, in \u001b[0;36msalvarDataFrameBancoDados\u001b[0;34m(df, tabela)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msalvarDataFrameBancoDados\u001b[39m(df,tabela):\n\u001b[1;32m     30\u001b[0m     df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc:postgresql://postgres-server:5432/projeto_imdb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.postgresql.Driver\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, tabela) \\\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostgres\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostgres\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m registros salvos na tabela \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdiretorioArquivo\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'diretorioArquivo' is not defined"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "    select ta.* \n",
    "    from title_akas ta \n",
    "        inner join title_br tbr \n",
    "        on ta.titleId=tbr.titleId\n",
    "    where 1=1\n",
    "    and region='BR' \n",
    "\"\"\"\n",
    "dfAkasBr = spark_client.sql(sql)\n",
    "salvarDataFrameBancoDados(dfAkasBr, \"title_akas_br\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "218967d3-a9bb-45d2-b003-908efc1d22ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1501476 entries, 0 to 1501475\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count    Dtype \n",
      "---  ------           --------------    ----- \n",
      " 0   titleId          1501476 non-null  object\n",
      " 1   ordering         1501476 non-null  object\n",
      " 2   title            1501476 non-null  object\n",
      " 3   region           1501476 non-null  object\n",
      " 4   language         1501476 non-null  object\n",
      " 5   types            1501476 non-null  object\n",
      " 6   attributes       1501476 non-null  object\n",
      " 7   isOriginalTitle  1501476 non-null  object\n",
      " 8   titleId          1501476 non-null  object\n",
      "dtypes: object(9)\n",
      "memory usage: 103.1+ MB\n"
     ]
    }
   ],
   "source": [
    "dfAkasBr.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
