{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtFHRxcH3nTR"
   },
   "source": [
    "![header-notebooks.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA/wAAACWCAYAAABjJoNaAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAABVgSURBVHgB7d17tJXlfSfw37s3cAAP93tAAUGjeEFFMF6CJrFJNMYxkKox167JqBmnnUmd6axp0umayepqm9WuyWirbRIjprHRak3S2NwNRkXxAmhUFLwhoIIg9+u57Lfvu48EhHP2PufAOZC3n89asPbled/97neff77P5fcko69vSgMAAAAolFIAAAAAhSPwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUUJ+AI1hjQxJzz4j4vbPTOHp4KdI0YvmalvjXZ8tx28KIbbvSAAAA4EDJ6OubJCaOSKMGRdx4RRJnH9v++4+viLjuu62xdquJKgAAAPuTlDhifeXSUodhPzdzUsSfzykHAAAABxL4OSKdOC7iopPqTz75wLsDAACAdgj8HJFmTkoCAACA7hP4OSKNGSzwAwAAHAyBnyPSirW7AwAAgO4T+DkizX+xTzS11B/lb24NAAAA2iHwc0Ravy2Jm35Vv2jfrQvsKgkAANAegZ8j1tcfSuPbC9NorbT//ncei/jaLwMAAIB2JKOvbzJEyhHtxLFJzDk9jePGRBb+01i1IYnbs7D/wpsBAABABwR+AAAAKCBT+gEAAKCA+sSRIk1jcENrjBnaNwb0qz6Nrbvz7dm2R5QHBt1XTnfFhJEDYlD/7HESsbMpYuW6bbErze/r4dnvvk/sjlOO7h/DskvY0RyxbE0lNu44PNcCAABQRIcl8JdLESeMifidaUlMG1uJk95VirFDkuhTam/CwVGxLQ/+6yNeWp/Efc+1xC+ej9je1PHkhMvPqMTU0bXD45otaXzr4e5NcJg+IeKSk9Noby3E5l1p/O387LOTrofX0Y1p/KfzImqtscg/dd4jSbyxuZ3zZ70kwwemceG0cpw7NeL4UWlMHpXEgL4D2jlTYzS1RqzeUIlXN5bi0VfS+NEzabz6Vuev+4sfiOzcHV/tT5cmsWjl3uejB0V86qwkPnZaxIRh/aO070elpXhhXSku/FpL7OmEOD77G/n46bVXnNy9OInl1vIDAAAcoHcDf5bdLjol4guzs6A/LqJvOX+xfuhubIg4eXz+L43/ML1cHQm++f7WuHNREpt2Hth+0840rpldrnnO1koS/7S4Ett2RZddd0ESH5rWfjDOZyY89GLEU6u7Xhph7oxSXD27dpvns5Hwr9134GdPH1/Jji3H+45P4qiGPZ9dO7z3y27RsaNK2b+oHnf9hUn8/Lk0/ub+iGdfj7o+e045hg2odPj+6k2RBf62a/noqRF/ekkSoxo7aJxd6rghaVtvx9uXPTXrrLhmdu3vsHBFZIFfGQoAAID99doa/inZaPMdVyfxd1cl1RHyvuVunyqGZaPYf3xxKe65NomTx7Ue8P7jr5Zj7Zba5yiX0rjs1K6PwufT4s+a1PH7+cD+x0+vRFf1z0bK85Hven72XBI7m/c+H5wN3n/pooh/+L0kLsk6U/aG/a7Lf5OLT846Qj6fxOfPi7bei0PgyjPT+IuPRcdhHwAAgEOuVwJ/PnX/x/+lFGdPjkNqSjYyfdc15Zg58Z2vb9wRccfj9Y+fe0ba5VD72fdEDK1TUmDujHI09u9aZ8KsSUl1Cnst23Yn8Y0H93YmjB+aff+rS3H1e5MYMvDQ/ZSNWafGly9O4n98qPzOaffdMOOYNP7sslI0NlifDwAA0Jt6PPCfOyXixiuSaOgbPWJgvyT+/xWlLPy+M7jfuagSW+tM1z9hbBIzJ3X+FvQppXHVWfWD61H90modga6Yc3r98nnzl1Viy6626807HW76RBInjOmZ6ez5tXxhdhq/e0b3z9/QJ7KR/bw2QwAAANDLejSKDWqI+MqltQu7HQp52M9Hkff12qYknlhZ+3MH9ou44szotPOmJjF+SOfafnJWEqVO3t2hAyrxwWn1G3/r4b2P/+QjSZx2dPSovLjiVy4tdfv3u+qsvPCekX0AAIDDoUcD/38+P5923zuBL19XP27wO4Pp1x+o/9m/c2IaQ/p3LtB2ZW1+vkvA+VM71/bS08rVWQG1PPNaGotfbWvz7rFJzD29d+5rPjPjc2dHtxw7IgAAADhMerRK/w9/nca5U1pj+tG1P2ZXc8QL6yIeWt4Sa7f1ic07o7p2fPCAJKaOqsRFJ6Ux/KjafRP5aP2Hp0XcunDva4+uSGPZ2lK8u8a096ED82n9Ud3qr5Yxg1rjwyd37Xb9x3OTmL+8fmfCnNP2KU3fgdseTX7TZM3mNP7piTSunFmKtEYNgkqaz3SIeOD53fHq5obYsD3fnSBfcpBk36clZh9fiukT6nccXHxyKW5+QCV8AACA3yY9GviXrslGov8+if/14Up87pxytTL+vrbsirhlQSVufzSNdVuzF5K8dP++bdoe/597I/7r+9Pqdni1nHtcKeZl59qTgfNw+51HW6vT0mv5zHvSLPDXPvdnz+7T5Z0F8o6EsUPygN5xm2OGNMdpE/pFLeu2tMYvluaP2r5H3iHyR/dELHi5El+6qJSF9wPD+E+XpnHzr9J49o0kmlry8+9/X0vxVz9P44NZJ8mffyyJkTUq6Ofb9h09PIlVGw5N6H9rRxKvvFmJ7U1J9Uoa+lRizJBSDBsQAAAAHCI9GvhzzZVS/N8fRdz7dEvcdFUpxg1pC9Yr1kd8el4lVm7Inyc1B7h3tyTx1Z9Fdc16XgSwIyeNT2JQQ1rtSNjjjieS+OOLkprr0POR7qOHRaza2H6b/JwfnpZP5+9a4u/fN+ITZ0b8v/s6bvPpc/pWt/Kr5ZfL+8SGHQcuJ/jBkxH3L6vEX85J4qKT2l7Lv8Ef3BHxL7/On9U5cfbBP3suYkRjVLfN60i+3CCfJbFqQ/eXEeSdL9/PrnfewjR+vbry9rXtud9tjwcPqNSvXAgAAECn9Fr99MWrSnHxjZGNOleqU/ivvj19O+x3Xh5uaxkxMI3G/fahb2qJuGdJ7ePyq7hqVsfvn398ElNGd3F4/22/OyOJ4Ue1/z3zooafmlX7HrRkl/7Nh1o7fD8f7b82u5f5iH++HeFf/CQP+10biX/gxdrvJ0lerLD7fyrbdqfxmVvT+MO787BfPWO77bbsTELiBwAAODR6dcO0DXkg/WkSH70pYtna6Jo0G2UeVzsM9iunMaCd2fF3PpHPEqh5aHz8jLbj2/Pp93Q/hI4fmhcUbL/D4bzjkhjYUPv4+56rxPI363/+nU+kcdnNafzjY13bDjA3cWj9Ywb2bYnu+t//ksRDLwUAAAC9qMen9Ldn+dr9gnWaT39Pq+u4823vhg1oiZGDytm/Ugzun8bghtaYOLIcp7yr9nnzkej21tk//VrE4pURZx/b8bGjB0WcM6UU9+9XZG9a1skwc2IclOsuKMWPnz2wM2HuafWPvf3R6LQVb+X/v7NzoJSkMaKxFBOyjodRg9IYPqASo4aUs3ucRGNDS4wdXI5TO1G4r5zkswy6PsvhsRVJ/OiZrndCAAAAcHB6PfAPbEjixDFpvHdqkgXwNAvypRgyIGJAn8o+a9n3XNaekNy96fR75NXqb3skjfdMTmqul7/izMgC/ztf+9Sstv3oO5KvTc8r4R8zvOM2p4yPOHZkxMvr9742bnAl3ndC7e+1ZksSC16OzknTmDwyjVmTSvGBEyKmjC7FqMaoLnHYe/35l9/zmWl06b6m3SvY970lldjZHAAAAPSyXgv8pVK+/VwSn5gZcebEPal7/8JtPedXL0QsezPihDEdtzn32Eq1qOAbb1fVHzM44iOn7nuNB1r6RsQN89P4myuTaKhxN6+ZncT/vGfveT55Vjn61FlQMe+RSrRU6t+X6RMivnB+EheeUNpnhsPh30Yv7wzZvwMFAACA3tEra/gb+7XGvM+U4qtz8rAfh8WOprzoX+02QwaW4tJT9z6/YGolhg6oHZz/7sGIny+NeOa12u3Om5LG8KPa2uSh/IMn1mweu5rTuOPx+mH/v70/a/f5vEp/0uVtA3va+q2tsW5bAAAAcBj0eOCfOiqNn/5BEucfn9acGt8bvvVw/ZnpeVX9fDu9ftlo/ZwZtRP0G1vyonppdSz9zkW1w/mEYUlcPqOtzcyJlXj32JrN40fP5lX3O77Ygf0i/mpuKb54Yan6+Ei0ZVcpmlsDAACAw6BHI3hjQ8SfXVaKCcOPjKHntVvSuPfp2m2mjo444+iIU8dHzJpUu+2tDzb/Zn3695dUquev5cqZbVPuP3pq/fsx7+Ha57r2vWnWOXFopu331OT/Hc1H2JQDAACAf0d6dA3/J2elcdak/FHn1+c3taTx+uYk3txSiTe3l+P1jfm08FK8sr5tZPya2QfXR/Gdx9K45JSOi/flL39oWsSIxrzCfcfn2b474mfL+saeuLy7NYnbH0/iDz/Q8TGTR6RxzrFZ4J8eNT25OuKZ1zv+8LxA4LXnd+0+tGaX+da2JFaub4otTf1i9cZKrNlciVWbyrGjKY1bPn3oayi0HP4yAgAAAP9u9WjgvzYL50mdHJmmady1qBK/XF6KB57fHdtb8o3p86SYH1iJfQv7TRoRB23JyohXNyYxaXjHafTKmUnd677v+TTrhNhznW3uWVyJ67IgXqt431fnJjGoIWq6dUFaLXjXkd9/X+0CgXvk2xHetTiNB17IrnVdfqn5teYH7jl53mmQxruG9mzBRAAAAHpfjwb+4UfVfn/rrogv/SDiB0/tGa2unYQnjTz4FQi7W7JR/oVpfPnijtvka/hrycP4LQ/nj94ZlF/blGTfJeLyGR0fO3Zw1PT6pnxHgY53BhjcP433Tsnfq30v/uHRiC9/v/J2yE9qTrIYUed3AgAA4LfPYS2j98OnoxqQOyufDn8o3JWNxOcF97pr/rJKPLnqwEBeyV767uNtnQrddcdjzTWL9R03Ookxg2v/bKs2JPHXv0ij7jSFt11wfAAAAFAwhzXwP7mq823zAnqTD8GU/tymHUk89GL3p7HfuiAvPd/+8YtfrcRTq7q3eD3fQeCep2pPuugTu6JU51dbuSli4/bolGEDk7jyTIvtAQAAiuawBv5RjZ0LmpNHJnHjlaXODlh3yld/0r1h+De2JLFwRY1Qnl3kNxZ070IfeTmNVRvrNKrUv+6hA/L7Wv/eDugb8ScfadsyEAAAgGI5rIH/iplJTBpWO5jOOCaqFeTHDj60o9BvbitVq+F31S0PtkRLpXabXy2vxFudHGHfI/9231xQv11L6ajqTIBapoyMuOTU2j/toH7NccMVScw5zeg+AABAEfVo0b56jhkWcdc1+XrzyEbNk1i3La0m3+EDkzhjQnNcMatvzJgY0b9Pz4TSv70/4huf6nz7zTuSuGNR/T6S3S1JfPuRNL54YedHzvOK+gteqt/uxXUR67emMWpwx+fOiw7ecHnEWZOTuPuJ1ljxVima0/w+JjFtXMTcMyJmT+kbIwflrY3uAwAAFFGPBv58JLxPnXw8Oguufzmn7XFrvll8lj/LpTyE1imVfwjkU+hf3VCKicM716Hwk6VpdWeBzpj3SMR1F0T06+Qd/t6SiF3N9dtt3pnEM2uSeF+dav/l7L5/5qz8X7m69WGlkka5vG+4F/QBAACKrEen9H/n0S41zwJpKQuqHV9SeogH+vPw/t3HKp1qm3de3Law8xewaWf2/R/rXNstWdt/XtL5c9/4y9boiiRJqve2Iyb1AwAAFE+PBv5bFuRF6A7NSHJTSxJ3Lzr00fSeJXnV/vrnnb8sjWdfjy65JwvxTZ3I5r94Ph+5j05btLIU9z7duY6KzvjnxQEAAEDB9GjgX7kh4ve/25yNYB9cUN/RFPFH30vj+bWHPvCv3dpWP6CWfHT/1k4U1Nvf06vTePyV+sH8Gw92fceAL30/iSdXHfz9yGdhfO2+AAAAoGB6vEr/ktXl+NxtbcG6O155K+Lz307je0t6buL5TfNrD8M/vyaJR1ZE1yVJfHthqbqGviO/fi2NpWvK0VX5koHPzYt4+OVSt6bk72yO+Oufp/GnP0yjuWsrBAAAAPgt0Cvb8i1aGXHxDWnMezhiw47OHbM7G/S+9eFKfOzmSix4OXrUU6+V4uka0/Vve6Q1Kt2cQf/jZ1ri2dc7juQ3zu/+koeNWej/xDdb47/fncZL6zp/XP57XP71NG6YH7/ZYjBf599VyUG+f/ASpQcBAAA6kIy+vqlXa7aNH5rGB0+MeP/xlZgypm+MbIxo6JOvdU9iU9YZ8PL6NO5b2hoLXinFs6+l1VHyPSaOiJg+IemweF/e9IEX0up5umryyIhJI9qPj4+tSGP77ui2k8a17Uawv/x7LHipko2wH3xsHdQ/ibMnR1w2PY0pI9OYMKIcR/XLb1QSW7NrX7m+JRavLse9T6exOAv8+47q9y1nHTInJ1Gp8ZewbG0llq9953Wed1wSfWt0GeWzEJas7P6f16jGJE4ZX7uoYL5sYv32AAAAYD+9HvgBAACAntcrU/oBAACA3iXwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABSTwAwAAQAEJ/AAAAFBAAj8AAAAUkMAPAAAABfRvjmRPSmgDRYIAAAAASUVORK5CYII=)\n",
    "\n",
    "Prática da **Aula 8: Spark e Fontes de Dados**, do curso de **Engenharia de Dados** da **[Awari](https://awari.com.br/)**. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.26.164-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.9/135.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting botocore<1.30.0,>=1.29.164\n",
      "  Downloading botocore-1.29.164-py3-none-any.whl (11.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.164->boto3) (1.26.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.164->boto3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.164->boto3) (1.16.0)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.26.164 botocore-1.29.164 jmespath-1.0.1 s3transfer-0.6.1\n",
      "Requirement already satisfied: pyspark in /usr/local/spark-3.3.1-bin-hadoop3/python (3.3.1)\n",
      "Collecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py4j\n",
      "Successfully installed py4j-0.10.9.5\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import date, datetime\n",
    "import pyspark\n",
    "import boto3\n",
    "\n",
    "from io import StringIO \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
    "from pyspark.sql.functions import col,array_contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "m2RXTtet4lm7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>AwariAula08</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x239ffb49510>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Criando uma sessão com o Spark que existe localmente(atualmente configurado junto com o JupyterLab)\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"spark://127.0.0.1:7077\") \\\n",
    "      .appName(\"AwariAula08\") \\\n",
    "      .config(\"spark.memory.offHeap.enabled\",\"true\") \\\n",
    "      .config(\"spark.memory.offHeap.size\",\"10g\") \\\n",
    "      .getOrCreate() \n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um RDD\n",
    "- Resilient Distributed Datasets, abstraem um conjunto de objetos distribuídos no cluster, geralmente executados na memória principal e podem depois serem salvos em um Haddop, Banco de Datos, arquivo, etc, quando você carrega \n",
    "- Um RDD ele é um dado IMUTAVEL e e semelhante a uma lista em Python, com a diferença de que o RDD é calculado em vários processos espalhados por vários servidores físicos, também chamados de nós em um cluster, enquanto uma coleção Python vive e processa em apenas um processo.\n",
    "- RDD é mais utilizado quando precisamos trabalhar mais baixo nivel manipulando diretamente o DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Partir da memoria\n",
    "dataList = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "rdd=spark.sparkContext.parallelize(dataList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Awari\\EngenhariaDados\\projeto-imdb\\venv\\Lib\\site-packages\\pyspark\\rdd.py:2836\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   2833\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2835\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 2836\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2838\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m   2839\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32m~\\Documents\\Awari\\EngenhariaDados\\projeto-imdb\\venv\\Lib\\site-packages\\pyspark\\context.py:2319\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   2317\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   2318\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2319\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\Documents\\Awari\\EngenhariaDados\\projeto-imdb\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\Documents\\Awari\\EngenhariaDados\\projeto-imdb\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\Documents\\Awari\\EngenhariaDados\\projeto-imdb\\venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n"
     ]
    }
   ],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,Male,19,15000,39,Healthcare,1,4',\n",
       " '2,Male,21,35000,81,Engineer,3,3',\n",
       " '3,Female,20,86000,6,Engineer,1,1',\n",
       " '4,Female,23,59000,77,Lawyer,0,2',\n",
       " '5,Female,31,38000,40,Entertainment,2,6']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A Partir de um arquivo\n",
    "rdd2 = spark.sparkContext.textFile(\"./arquivos/customers.csv\")\n",
    "header = rdd2.first()\n",
    "rdd2 = rdd2.filter(lambda row: row != header)\n",
    "rdd2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Operações\n",
    "Transformações no Spark etorna outro RDD e essas transformações são lentas, o que significa que elas não são executadas até que você chame uma ação no RDD. Algumas transformações em RDD's são flatMap(), map(), reduceByKey(), filter(), sortByKey() e retornam um novo RDD ao invés de atualizar o atual.\n",
    "\n",
    "# RDD Ações\n",
    "Retorna os valores de um RDD para um nó de driver. Em outras palavras, qualquer função RDD que retorna um não RDD é considerada uma ação. Algumas ações no RDD são count(), collect(), first(), max(), reduce() and more.\n",
    "\n",
    "# PySpark DataFrame\n",
    "DataFrame é uma coleção distribuída de dados organizados em colunas nomeadas. É conceitualmente equivalente a uma tabela em um banco de dados relacional ou um quadro de dados em R/Python, mas com otimizações mais ricas sob o capô. Os DataFrames podem ser construídos a partir de uma ampla variedade de fontes, como arquivos de dados estruturados, tabelas no Hive, bancos de dados externos ou RDDs existentes. Diferente do RDD é mais utilizado para analise de dados, debugs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "# Criando um DataFrame\n",
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "print(df.show(), df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+\n",
      "|CustomerID|Gender|Age|Annual Income ($)|Spending Score (1-100)|   Profession|Work Experience|Family Size|\n",
      "+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+\n",
      "|         1|  Male| 19|            15000|                    39|   Healthcare|              1|          4|\n",
      "|         2|  Male| 21|            35000|                    81|     Engineer|              3|          3|\n",
      "|         3|Female| 20|            86000|                     6|     Engineer|              1|          1|\n",
      "|         4|Female| 23|            59000|                    77|       Lawyer|              0|          2|\n",
      "|         5|Female| 31|            38000|                    40|Entertainment|              2|          6|\n",
      "|         6|Female| 22|            58000|                    76|       Artist|              0|          2|\n",
      "|         7|Female| 35|            31000|                     6|   Healthcare|              1|          3|\n",
      "|         8|Female| 23|            84000|                    94|   Healthcare|              1|          3|\n",
      "|         9|  Male| 64|            97000|                     3|     Engineer|              0|          3|\n",
      "|        10|Female| 30|            98000|                    72|       Artist|              1|          4|\n",
      "|        11|  Male| 67|             7000|                    14|     Engineer|              1|          3|\n",
      "|        12|Female| 35|            93000|                    99|   Healthcare|              4|          4|\n",
      "|        13|Female| 58|            80000|                    15|    Executive|              0|          5|\n",
      "|        14|Female| 24|            91000|                    77|       Lawyer|              1|          1|\n",
      "|        15|  Male| 37|            19000|                    13|       Doctor|              0|          1|\n",
      "|        16|  Male| 22|            51000|                    79|   Healthcare|              1|          2|\n",
      "|        17|Female| 35|            29000|                    35|    Homemaker|              9|          5|\n",
      "|        18|  Male| 20|            89000|                    66|   Healthcare|              1|          6|\n",
      "|        19|  Male| 52|            20000|                    29|Entertainment|              1|          4|\n",
      "|        20|Female| 35|            62000|                    98|       Artist|              0|          1|\n",
      "+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Annual Income ($): string (nullable = true)\n",
      " |-- Spending Score (1-100): string (nullable = true)\n",
      " |-- Profession: string (nullable = true)\n",
      " |-- Work Experience: string (nullable = true)\n",
      " |-- Family Size: string (nullable = true)\n",
      "\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "# Lendo um arquivo\n",
    "df = spark.read.option(\"header\", True).csv(\"./arquivos/customers.csv\")\n",
    "print(df.show(), df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquivos suportados por um PySpark DataFrame\n",
    "\n",
    "- csv\n",
    "- text\n",
    "- Avro\n",
    "- Parquet\n",
    "- tsv\n",
    "- xml e muito mais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+\n",
      "|CustomerID|Gender|Age|Annual Income ($)|Spending Score (1-100)|   Profession|Work Experience|Family Size|\n",
      "+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+\n",
      "|         1|  Male| 19|            15000|                    39|   Healthcare|              1|          4|\n",
      "|         2|  Male| 21|            35000|                    81|     Engineer|              3|          3|\n",
      "|         3|Female| 20|            86000|                     6|     Engineer|              1|          1|\n",
      "|         4|Female| 23|            59000|                    77|       Lawyer|              0|          2|\n",
      "|         5|Female| 31|            38000|                    40|Entertainment|              2|          6|\n",
      "+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+\n",
      "\n",
      "root\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Annual Income ($): string (nullable = true)\n",
      " |-- Spending Score (1-100): string (nullable = true)\n",
      " |-- Profession: string (nullable = true)\n",
      " |-- Work Experience: string (nullable = true)\n",
      " |-- Family Size: string (nullable = true)\n",
      "\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"CUSTOMERS\")\n",
    "df2 = spark.sql(\"SELECT * from CUSTOMERS LIMIT 5\")\n",
    "print(df2.show(), df2.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|   Profession|total|\n",
      "+-------------+-----+\n",
      "|Entertainment|  234|\n",
      "|   Healthcare|  339|\n",
      "|       Lawyer|  142|\n",
      "|         null|   35|\n",
      "|    Homemaker|   60|\n",
      "|    Executive|  153|\n",
      "|       Artist|  612|\n",
      "|    Marketing|   85|\n",
      "|       Doctor|  161|\n",
      "|     Engineer|  179|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usando GROUPBY\n",
    "groupDF = spark.sql(\"SELECT Profession, count(*) as total from CUSTOMERS group by Profession\")\n",
    "groupDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "\n",
    "# Criando uma sessão com o Spark que existe localmente(atualmente configurado junto com o JupyterLab)\n",
    "conf.setMaster(\"local[1]\") \n",
    "conf.set(\"spark.driver.host\", \"awari-jupyterlab\") \\\n",
    "    .set(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\") \\\n",
    "    .set(\"parquet.enable.summary-metadata\", \"false\") \\\n",
    "    .set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\") \\\n",
    "    .set(\"spark.driver.port\", \"20020\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", 'awari-nginx:9000') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint.region\", 'sa-east-1') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", 'mnYOiUf07UBjjJwf') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", '1Qu7X3EmbIYDNXUiuvFSDUJwJ4fWdyT5') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .set(\"spark.hadoop.com.amazonaws.services.s3.enableV2\", \"true\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.committer.staging.conflict-mode\", \"replace\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.committer.name\", \"directory\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.committer.staging.tmp.path\", \"/tmp/staging\")\n",
    "\n",
    "conf.setAppName('AwariAula08-S33')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<botocore.client.S3 at 0x7fe23f6a69b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = boto3.client('s3', \n",
    "    endpoint_url='http://awari-minio-nginx:9000',\n",
    "    aws_access_key_id='mnYOiUf07UBjjJwf',\n",
    "    aws_secret_access_key='1Qu7X3EmbIYDNXUiuvFSDUJwJ4fWdyT5',\n",
    "    aws_session_token=None,\n",
    "    config=boto3.session.Config(signature_version='s3v4'),\n",
    "    verify=False,\n",
    "    region_name='sa-east-1'\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", True).csv(\"s3a://aula-08/resources/usuarios.csv\")\n",
    "print(df.show(), df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Usando RDD\n",
    "# A Partir de um arquivo\n",
    "rdd3 = spark.sparkContext.textFile(\"s3a://aula-08/resources/usuarios.csv\")\n",
    "rdd3 = rdd3.map( lambda f: f.split(\",\"))\n",
    "header = rdd3.first()\n",
    "\n",
    "rdd3 = rdd3.filter(lambda row: row != header)\n",
    "rdd3 = sc.parallelize(rdd3.collect())\n",
    "df = rdd3.toDF(header)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convertendo CSV para PARQUET e salvando no storage\n",
    "df.write.save(\"s3a://aula-08/meu-parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lendo um parquet file\n",
    "parDF1=spark.read.parquet(\"s3a://aula-08/meu-parquet\")\n",
    "parDF1.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame\n",
    "Diferente de Series, **DataFrame** possui uma estrutura bidemensial, como uma planilha de Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'macas': [3, 2, 0, 1], \n",
    "    'laranjas': [0, 3, 7, 2]\n",
    "}\n",
    "\n",
    "compras = pd.DataFrame(data)\n",
    "\n",
    "compras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compras = pd.DataFrame(data, index=['Alex', 'Roberto', 'Bernardo', 'Paulo'])\n",
    "\n",
    "compras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para extrair apenas uma Serie\n",
    "compras.loc['Alex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lendo informações de um CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('arquivos/customers.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apenas os 5 primeiros registros\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quando carregamos um CSV os indices são gerados aleatoriamente por ordem da linha \n",
    "# caso queiramos especificar um indice, precisaremos informar qual a coluna é o indice\n",
    "\n",
    "df = pd.read_csv('arquivos/customers.csv', index_col=0)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lendo arquivos em JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('./arquivos/estados.json', orient='records')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo dados de uma URL\n",
    "url = \"https://api.exchangerate-api.com/v4/latest/USD\"\n",
    "df = pd.read_json(url)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvando DataFrame em JSON ou CSV\n",
    "\n",
    "Podemos, usando a mesma lógica, carregar um arquivo JSON salvar em CSV e vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simples formato\n",
    "df = pd.DataFrame([1,2,3])\n",
    "print(df)\n",
    "df.to_json(\"./arquivos/numeros_json.json\")\n",
    "df.to_csv(\"./arquivos/numeros_csv.json\")\n",
    "!ls ./arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([1,2,3], columns=['Numeros'])\n",
    "df.to_json(\"./arquivos/numeros_json2.json\")\n",
    "df.to_csv(\"./arquivos/numeros_csv2.csv\")\n",
    "!ls ./arquivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratando um arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./arquivos/4300Answers.csv')\n",
    "df.head(2) # Checar colunas com NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo colunas NaN para \"-\"\n",
    "df = df.fillna(\"-\") # Nós precisamos atrelar o resultado a variavel principal\n",
    "df.head(2) # Checar colunas novamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retornando apenas o sexo Feminino\n",
    "resultado_query=df.query(\"`Qual seu sexo?` == 'Feminino'\")\n",
    "resultado_query.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando novo Json apenas com os resultados femininos\n",
    "resultado_query.to_csv(\"./arquivos/4300Answers_femininos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo arquivo gerado\n",
    "df = pd.read_csv(\"./arquivos/4300Answers_femininos.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAH7iNcj4PEE"
   },
   "source": [
    "---\n",
    "\n",
    "Notebook utilizado para fins educacionais da **Awari**.\n",
    "\n",
    "**© AWARI. Todos os direitos reservados.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMpmB1/ZiTM76Kc2DEBlVuc",
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1WPJzzm-0hWc6Q5JetcdpWbJrsPesiWLs",
     "timestamp": 1666129304753
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
