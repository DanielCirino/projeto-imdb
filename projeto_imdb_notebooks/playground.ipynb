{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4f047a-5cde-4535-b711-499ab3158b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import boto3\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08640c07-2eb0-4a7b-87b4-6287a3905307",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format=\"[%(levelname)s] [%(asctime)s] %(message)s\",\n",
    "                    level=logging.WARN,\n",
    "                    datefmt=\"%d/%m/%y %H:%M:%S\",\n",
    "                    encoding=\"utf-8\")\n",
    "\n",
    "AWS_ENDPOINT = \"http://minio-webserver:9000\"\n",
    "AWS_ACCESS_KEY_ID = \"U6dHDkvTv3CdrviA\"\n",
    "AWS_SECRET_ACCESS_KEY = \"o8LTVjrVtNuzOv7DjGOxhN0HQDeksEej\"\n",
    "AWS_REGION =\"us-east-1\"\n",
    "\n",
    "SPARK_APP_NAME= \"projeto-imdb-notebook\"\n",
    "SPARK_MASTER_URL=\"spark://spark-master:7077\"\n",
    "\n",
    "BUCKET_ANALYTICS=\"projeto-imdb-analytics\"\n",
    "BUCKET_STAGE=\"projeto-imdb-stage\"\n",
    "\n",
    "\n",
    "def obterJars():\n",
    "    dir = \"/notebooks/spark-jars\"\n",
    "    jars = os.listdir(dir)\n",
    "    stringJars = \"\"\n",
    "\n",
    "    for jar in jars:\n",
    "        logging.info(f\"{dir}/{jar}\")\n",
    "        stringJars += f\"{dir}/{jar},\"\n",
    "\n",
    "    return stringJars[:-1]\n",
    "\n",
    "def salvarDataFrameBancoDados(df,tabela):\n",
    "    df.write.format(\"jdbc\")\\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres-server:5432/projeto_imdb\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", tabela) \\\n",
    "    .option(\"user\", \"postgres\").option(\"password\", \"postgres\").save(mode=\"overwrite\")\n",
    "\n",
    "    print(f\"{df.count()} registros salvos na tabela {tabela}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b169ddc2-8238-4201-b3f4-88fa82296918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/25 19:41:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b17dd26c62c6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>projeto-imdb-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f34672d4910>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spark_client.stop()\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setAppName(SPARK_APP_NAME)\n",
    "conf.setMaster(SPARK_MASTER_URL)\n",
    "\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", AWS_ENDPOINT) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint.region\", AWS_REGION) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.access.key\",AWS_ACCESS_KEY_ID) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", False) \\\n",
    "        .set(\"spark.hadoop.com.amazonaws.services.s3.enableV2\", True) \\\n",
    "        .set(\"spark.jars\", obterJars()) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.committer.staging.conflict-mode\", \"replace\") \\\n",
    "        .set(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "        .set(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .set(\"spark.sql.sources.commitProtocolClass\",\n",
    "             \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\") \\\n",
    "        .set(\"spark.executor.memory\", \"2g\") \\\n",
    "        .set(\"spark.driver.memory\", \"2g\") \\\n",
    "        .set(\"spark.cores.max\", \"4\") \n",
    "        # .set(\"spark.hadoop.fs.s3a.committer.name\", \"magic\") \\\n",
    "        # .set(\"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\", \"true\")\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf).getOrCreate()\n",
    "sc.setLogLevel(\"warn\")\n",
    "spark_client = SparkSession(sc)\n",
    "spark_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c288c4-2d0a-4465-9fba-5a4ccc0a0e6b",
   "metadata": {},
   "source": [
    "# Criar as tabelas temporárias a partir dos arquivos *.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f9983de-435b-4128-b5ee-23356ea84486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/25 19:43:53 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nconst: string (nullable = true)\n",
      " |-- primaryName: string (nullable = true)\n",
      " |-- birthYear: string (nullable = true)\n",
      " |-- deathYear: string (nullable = true)\n",
      " |-- primaryProfession: string (nullable = true)\n",
      " |-- knownForTitles: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNameBasics = spark_client \\\n",
    "        .read \\\n",
    "        .parquet(f\"s3a://{BUCKET_ANALYTICS}/name_basics\", header=True)\n",
    "\n",
    "dfNameBasics.createOrReplaceTempView(\"name_basics\")\n",
    "dfNameBasics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fad49ccb-4fe1-452c-bfdd-4efd9e3c81fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- titleId: string (nullable = true)\n",
      " |-- ordering: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- types: string (nullable = true)\n",
      " |-- attributes: string (nullable = true)\n",
      " |-- isOriginalTitle: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfAkas = spark_client \\\n",
    "        .read \\\n",
    "        .parquet(f\"s3a://{BUCKET_ANALYTICS}/title_akas\", header=True)\n",
    "\n",
    "dfAkas.createOrReplaceTempView(\"title_akas\")\n",
    "dfAkas.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0f14216-8b7d-4061-990f-5a353bdda96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- titleType: string (nullable = true)\n",
      " |-- primaryTitle: string (nullable = true)\n",
      " |-- originalTitle: string (nullable = true)\n",
      " |-- isAdult: string (nullable = true)\n",
      " |-- startYear: string (nullable = true)\n",
      " |-- endYear: string (nullable = true)\n",
      " |-- runtimeMinutes: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTitleBasics = spark_client \\\n",
    "        .read \\\n",
    "        .parquet(f\"s3a://{BUCKET_ANALYTICS}/title_basics\", header=True)\n",
    "\n",
    "dfTitleBasics.createOrReplaceTempView(\"title_basics\")\n",
    "dfTitleBasics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0b56269-f55c-4b6e-8b8f-5bc6320c59e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- directors: string (nullable = true)\n",
      " |-- writers: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTitleCrew = spark_client \\\n",
    "        .read \\\n",
    "        .parquet(f\"s3a://{BUCKET_ANALYTICS}/title_crew\", header=True)\n",
    "\n",
    "dfTitleCrew.createOrReplaceTempView(\"title_crew\")\n",
    "dfTitleCrew.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "696ab5c7-51fc-4c2e-a91b-97095586bf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- parentTconst: string (nullable = true)\n",
      " |-- seasonNumber: string (nullable = true)\n",
      " |-- episodeNumber: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTitleEpisode = spark_client \\\n",
    "        .read \\\n",
    "        .parquet(f\"s3a://{BUCKET_ANALYTICS}/title_episode\", header=True)\n",
    "\n",
    "dfTitleEpisode.createOrReplaceTempView(\"title_episode\")\n",
    "dfTitleEpisode.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "109a5191-cb36-4d93-9a18-e624a773c9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- ordering: string (nullable = true)\n",
      " |-- nconst: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- characters: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTitlePrincipals = spark_client \\\n",
    "        .read \\\n",
    "        .parquet(f\"s3a://{BUCKET_ANALYTICS}/title_principals\", header=True)\n",
    "\n",
    "dfTitlePrincipals.createOrReplaceTempView(\"title_principals\")\n",
    "dfTitlePrincipals.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "198a1f90-0dfb-4a75-94c4-caff96d705bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- averageRating: string (nullable = true)\n",
      " |-- numVotes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfRatings = spark_client \\\n",
    "        .read \\\n",
    "        .parquet(f\"s3a://{BUCKET_ANALYTICS}/title_ratings\", header=True)\n",
    "\n",
    "dfRatings.createOrReplaceTempView(\"title_ratings\")\n",
    "dfRatings.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efdd61c-b98d-4ba6-96c2-2d8a32e66ce3",
   "metadata": {},
   "source": [
    "# Extrair dados da região do Brasil para análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc9d9cc6-7ef7-4344-9cab-a579e3546e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:===================================>                       (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existem 111159 titulos que foram lançados no Brasil\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Títulos que possuem lançamentos no Brasil\n",
    "sqlTitulosBR = \"\"\"\n",
    "    select distinct titleId\n",
    "    from title_akas \n",
    "    where 1=1\n",
    "    and region='BR' \n",
    "    --group by titleId having count(*)=1\n",
    "    order by titleId\n",
    "\"\"\"\n",
    "\n",
    "dfTitulosBR = spark_client.sql(sqlTitulosBR)\n",
    "dfTitulosBR.createOrReplaceTempView(\"title_br\")\n",
    "print(f\"Existem {dfTitulosBR.count()} titulos que foram lançados no Brasil\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc56a809-8013-4199-b382-9e9e63d95f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119471 registros salvos na tabela title_akas_br]\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "    select ta.* \n",
    "    from title_akas ta \n",
    "        inner join title_br tbr \n",
    "        on ta.titleId=tbr.titleId\n",
    "    where 1=1\n",
    "    and region='BR' \n",
    "\"\"\"\n",
    "dfAkasBr = spark_client.sql(sql)\n",
    "salvarDataFrameBancoDados(dfAkasBr, \"title_akas_br\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47eba687-bbea-42b7-8518-4776a9c00cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111109 registros salvos na tabela title_basics_br]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "    select tb.* \n",
    "    from title_basics tb \n",
    "        inner join title_br tbr \n",
    "        on tb.tconst=tbr.titleId\n",
    "\"\"\"\n",
    "dfBasicsBr = spark_client.sql(sql)\n",
    "salvarDataFrameBancoDados(dfBasicsBr, \"title_basics_br\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38d245d0-679a-410c-913a-af4faec51f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111118 registros salvos na tabela title_crew_br]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "    select tc.* \n",
    "    from title_crew tc \n",
    "        inner join title_br tbr \n",
    "        on tc.tconst=tbr.titleId\n",
    "\"\"\"\n",
    "dfCrewBr = spark_client.sql(sql)\n",
    "salvarDataFrameBancoDados(dfCrewBr, \"title_crew_br\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5089d30-c3f3-42e4-9b09-c1a82325fef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5868 registros salvos na tabela title_episode_br]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "    select te.* \n",
    "    from title_episode te \n",
    "        inner join title_br tbr \n",
    "        on te.tconst=tbr.titleId\n",
    "\"\"\"\n",
    "dfEpisodeBr = spark_client.sql(sql)\n",
    "salvarDataFrameBancoDados(dfEpisodeBr, \"title_episode_br\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11844691-2bb7-4bd3-8860-00ed5d3a9b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 84:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "930936 registros salvos na tabela title_principals_br]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "    select tp.* \n",
    "    from title_principals tp \n",
    "        inner join title_br tbr \n",
    "        on tp.tconst=tbr.titleId\n",
    "\"\"\"\n",
    "dfPrincipalsBr = spark_client.sql(sql)\n",
    "salvarDataFrameBancoDados(dfPrincipalsBr, \"title_principals_br\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a99fb247-a904-4faf-8137-41cf4011eca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 95:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81679 registros salvos na tabela title_ratings_br]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "    select tr.* \n",
    "    from title_ratings tr \n",
    "        inner join title_br tbr \n",
    "        on tr.tconst=tbr.titleId\n",
    "\"\"\"\n",
    "dfRatingsBr = spark_client.sql(sql)\n",
    "salvarDataFrameBancoDados(dfRatingsBr, \"title_ratings_br\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "218967d3-a9bb-45d2-b003-908efc1d22ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdfAkasBr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/dataframe.py:2977\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2944\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   2945\u001b[0m \n\u001b[1;32m   2946\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2974\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[1;32m   2975\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 2977\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   2978\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   2979\u001b[0m     )\n\u001b[1;32m   2980\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'info'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/25 21:12:58 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "23/07/25 21:12:58 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:978)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "dfAkasBr.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
